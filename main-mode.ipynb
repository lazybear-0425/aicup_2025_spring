{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "82f900b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e37999",
   "metadata": {},
   "source": [
    "## Feature Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "79810da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT(xreal, ximag):    \n",
    "    n = 2\n",
    "    while(n*2 <= len(xreal)):\n",
    "        n *= 2\n",
    "    \n",
    "    p = int(math.log(n, 2))\n",
    "    \n",
    "    for i in range(0, n):\n",
    "        a = i\n",
    "        b = 0\n",
    "        for j in range(0, p):\n",
    "            b = int(b*2 + a%2)\n",
    "            a = a//2\n",
    "        if(b > i):\n",
    "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
    "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
    "            \n",
    "    wreal = []\n",
    "    wimag = []\n",
    "        \n",
    "    arg = float(-2 * math.pi / n)\n",
    "    treal = float(math.cos(arg))\n",
    "    timag = float(math.sin(arg))\n",
    "    \n",
    "    wreal.append(float(1.0))\n",
    "    wimag.append(float(0.0))\n",
    "    \n",
    "    for j in range(1, int(n/2)):\n",
    "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
    "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
    "        \n",
    "    m = 2\n",
    "    while(m < n + 1):\n",
    "        for k in range(0, n, m):\n",
    "            for j in range(0, int(m/2), 1):\n",
    "                index1 = k + j\n",
    "                index2 = int(index1 + m / 2)\n",
    "                t = int(n * j / m)\n",
    "                treal = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
    "                timag = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
    "                ureal = xreal[index1]\n",
    "                uimag = ximag[index1]\n",
    "                xreal[index1] = ureal + treal\n",
    "                ximag[index1] = uimag + timag\n",
    "                xreal[index2] = ureal - treal\n",
    "                ximag[index2] = uimag - timag\n",
    "        m *= 2\n",
    "        \n",
    "    return n, xreal, ximag   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8dc15334",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_data(input_data, swinging_times):   \n",
    "    txtlength = swinging_times[-1] - swinging_times[0]\n",
    "    a_mean = [0] * txtlength\n",
    "    g_mean = [0] * txtlength\n",
    "       \n",
    "    for num in range(len(swinging_times)-1):\n",
    "        a = []\n",
    "        g = []\n",
    "        for swing in range(swinging_times[num], swinging_times[num+1]):\n",
    "            a.append(math.sqrt(math.pow((input_data[swing][0] + input_data[swing][1] + input_data[swing][2]), 2)))\n",
    "            g.append(math.sqrt(math.pow((input_data[swing][3] + input_data[swing][4] + input_data[swing][5]), 2)))\n",
    "\n",
    "        a_mean[num] = (sum(a) / len(a))\n",
    "        g_mean[num] = (sum(g) / len(g)) # flag\n",
    "    \n",
    "    return a_mean, g_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "455afc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(input_data, swinging_now, swinging_times, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer):\n",
    "    allsum = []\n",
    "    mean = []\n",
    "    var = []\n",
    "    rms = []\n",
    "    XYZmean_a = 0\n",
    "    a = []\n",
    "    g = []\n",
    "    a_s1 = 0\n",
    "    a_s2 = 0\n",
    "    g_s1 = 0\n",
    "    g_s2 = 0\n",
    "    a_k1 = 0\n",
    "    a_k2 = 0\n",
    "    g_k1 = 0\n",
    "    g_k2 = 0\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        if i==0:\n",
    "            allsum = input_data[i]\n",
    "            a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
    "            g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
    "            continue\n",
    "        \n",
    "        a.append(math.sqrt(math.pow((input_data[i][0] + input_data[i][1] + input_data[i][2]), 2)))\n",
    "        g.append(math.sqrt(math.pow((input_data[i][3] + input_data[i][4] + input_data[i][5]), 2)))\n",
    "       \n",
    "        allsum = [allsum[feature_index] + input_data[i][feature_index] for feature_index in range(len(input_data[i]))]\n",
    "        \n",
    "    mean = [allsum[feature_index] / len(input_data) for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        if i==0:\n",
    "            var = input_data[i]\n",
    "            rms = input_data[i]\n",
    "            continue\n",
    "\n",
    "        var = [var[feature_index] + math.pow((input_data[i][feature_index] - mean[feature_index]), 2) for feature_index in range(len(input_data[i]))]\n",
    "        rms = [rms[feature_index] + math.pow(input_data[i][feature_index], 2) for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    # flag\n",
    "    var = [var[feature_index] if var[feature_index] > 0 else 0 for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    var = [math.sqrt((var[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
    "    rms = [math.sqrt((rms[feature_index] / len(input_data))) for feature_index in range(len(input_data[i]))]\n",
    "    \n",
    "    a_max = [max(a)]\n",
    "    a_min = [min(a)]\n",
    "    a_mean = [sum(a) / len(a)]\n",
    "    g_max = [max(g)]\n",
    "    g_min = [min(g)]\n",
    "    g_mean = [sum(g) / len(g)]\n",
    "    \n",
    "    a_var = math.sqrt(math.pow((var[0] + var[1] + var[2]), 2))\n",
    "    \n",
    "    for i in range(len(input_data)):\n",
    "        a_s1 = a_s1 + math.pow((a[i] - a_mean[0]), 4)\n",
    "        a_s2 = a_s2 + math.pow((a[i] - a_mean[0]), 2)\n",
    "        g_s1 = g_s1 + math.pow((g[i] - g_mean[0]), 4)\n",
    "        g_s2 = g_s2 + math.pow((g[i] - g_mean[0]), 2)\n",
    "        a_k1 = a_k1 + math.pow((a[i] - a_mean[0]), 3)\n",
    "        g_k1 = g_k1 + math.pow((g[i] - g_mean[0]), 3)\n",
    "    \n",
    "    a_s1 = a_s1 / len(input_data)\n",
    "    a_s2 = a_s2 / len(input_data)\n",
    "    g_s1 = g_s1 / len(input_data)\n",
    "    g_s2 = g_s2 / len(input_data)\n",
    "    a_k2 = math.pow(a_s2, 1.5)\n",
    "    g_k2 = math.pow(g_s2, 1.5)\n",
    "    a_s2 = a_s2 * a_s2\n",
    "    g_s2 = g_s2 * g_s2\n",
    "    \n",
    "    a_kurtosis = [a_s1 / a_s2]\n",
    "    g_kurtosis = [g_s1 / g_s2]\n",
    "    a_skewness = [a_k1 / a_k2]\n",
    "    g_skewness = [g_k1 / g_k2]\n",
    "    \n",
    "    a_fft_mean = 0\n",
    "    g_fft_mean = 0\n",
    "    cut = int(n_fft / swinging_times)\n",
    "    a_psd = []\n",
    "    g_psd = []\n",
    "    entropy_a = []\n",
    "    entropy_g = []\n",
    "    e1 = []\n",
    "    e3 = []\n",
    "    e2 = 0\n",
    "    e4 = 0\n",
    "    \n",
    "    for i in range(cut * swinging_now, cut * (swinging_now + 1)):\n",
    "        a_fft_mean += a_fft[i]\n",
    "        g_fft_mean += g_fft[i]\n",
    "        a_psd.append(math.pow(a_fft[i], 2) + math.pow(a_fft_imag[i], 2))\n",
    "        g_psd.append(math.pow(g_fft[i], 2) + math.pow(g_fft_imag[i], 2))\n",
    "        e1.append(math.pow(a_psd[-1], 0.5))\n",
    "        e3.append(math.pow(g_psd[-1], 0.5))\n",
    "        \n",
    "    a_fft_mean = a_fft_mean / cut\n",
    "    g_fft_mean = g_fft_mean / cut\n",
    "    \n",
    "    a_psd_mean = sum(a_psd) / len(a_psd)\n",
    "    g_psd_mean = sum(g_psd) / len(g_psd)\n",
    "    \n",
    "    for i in range(cut):\n",
    "        e2 += math.pow(a_psd[i], 0.5)\n",
    "        e4 += math.pow(g_psd[i], 0.5)\n",
    "    \n",
    "    for i in range(cut):\n",
    "        entropy_a.append((e1[i] / e2) * math.log(e1[i] / e2))\n",
    "        entropy_g.append((e3[i] / e4) * math.log(e3[i] / e4))\n",
    "    \n",
    "    a_entropy_mean = sum(entropy_a) / len(entropy_a)\n",
    "    g_entropy_mean = sum(entropy_g) / len(entropy_g)       \n",
    "        \n",
    "    \n",
    "    output = mean + var + rms + a_max + a_mean + a_min + g_max + g_mean + g_min + [a_fft_mean] + [g_fft_mean] + [a_psd_mean] + [g_psd_mean] + a_kurtosis + g_kurtosis + a_skewness + g_skewness + [a_entropy_mean] + [g_entropy_mean]\n",
    "    writer.writerow(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "51d9705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generate(datapath = './train_data', tar_dir = 'tabular_data_train'):\n",
    "    pathlist_txt = Path(datapath).glob('**/*.txt')\n",
    "\n",
    "    \n",
    "    for file in pathlist_txt:\n",
    "        f = open(file)\n",
    "\n",
    "        All_data = []\n",
    "\n",
    "        count = 0\n",
    "        for line in f.readlines():\n",
    "            if line == '\\n' or count == 0:\n",
    "                count += 1\n",
    "                continue\n",
    "            num = line.split(' ')\n",
    "            if len(num) > 5:\n",
    "                tmp_list = []\n",
    "                for i in range(6):\n",
    "                    tmp_list.append(int(num[i]))\n",
    "                All_data.append(tmp_list)\n",
    "        \n",
    "        f.close()\n",
    "\n",
    "        swing_index = np.linspace(0, len(All_data), 28, dtype = int)\n",
    "        # filename.append(int(Path(file).stem))\n",
    "        # all_swing.append([swing_index])\n",
    "\n",
    "        headerList = ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'ax_var', 'ay_var', 'az_var', 'gx_var', 'gy_var', 'gz_var', 'ax_rms', 'ay_rms', 'az_rms', 'gx_rms', 'gy_rms', 'gz_rms', 'a_max', 'a_mean', 'a_min', 'g_max', 'g_mean', 'g_min', 'a_fft', 'g_fft', 'a_psd', 'g_psd', 'a_kurt', 'g_kurt', 'a_skewn', 'g_skewn', 'a_entropy', 'g_entropy']                \n",
    "        \n",
    "\n",
    "        with open('./{dir}/{fname}.csv'.format(dir = tar_dir, fname = Path(file).stem), 'w', newline = '') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(headerList)\n",
    "            # try:\n",
    "            #     a_fft, g_fft = FFT_data(All_data, swing_index)\n",
    "            #     a_fft_imag = [0] * len(a_fft)\n",
    "            #     g_fft_imag = [0] * len(g_fft)\n",
    "            #     n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
    "            #     n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
    "            #     for i in range(len(swing_index)):\n",
    "            #         if i==0:\n",
    "            #             continue\n",
    "            #         feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
    "            # except:\n",
    "            #     print(Path(file).stem)\n",
    "            #     continue\n",
    "            a_fft, g_fft = FFT_data(All_data, swing_index)\n",
    "            a_fft_imag = [0] * len(a_fft)\n",
    "            g_fft_imag = [0] * len(g_fft)\n",
    "            n_fft, a_fft, a_fft_imag = FFT(a_fft, a_fft_imag)\n",
    "            n_fft, g_fft, g_fft_imag = FFT(g_fft, g_fft_imag)\n",
    "            for i in range(len(swing_index)):\n",
    "                if i==0:\n",
    "                    continue\n",
    "                feature(All_data[swing_index[i-1]: swing_index[i]], i - 1, len(swing_index) - 1, n_fft, a_fft, g_fft, a_fft_imag, g_fft_imag, writer)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a541c5a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ff6a3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_corr(X_data, y_data, name=''):\n",
    "    corr_gender = X_data.apply(lambda col: col.corr(y_data[name]))\n",
    "    corr_gender = corr_gender.sort_values(ascending=False)\n",
    "    print(name, 'corr')\n",
    "    print(corr_gender.head(10))  # Top 10 正相關\n",
    "    print(corr_gender.tail(10))  # Top 10 負相關\n",
    "\n",
    "    # display(corr_gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "6b99c26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bd7a1c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_binary(X_train, y_train, X_test, y_test, group_size=27, n_estimators=100):\n",
    "    # clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, class_weight='balanced')\n",
    "    # clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    # clf = HistGradientBoostingClassifier(random_state=42)\n",
    "    # clf = MLPClassifier()\n",
    "    num_class = [(y_train == 0).sum(), (y_train == 1).sum()]\n",
    "    num_class = {i: len(y_train) / num for i, num in enumerate(num_class)}\n",
    "    sample_weight = np.zeros(len(y_train))\n",
    "    for i in range(2):\n",
    "        sample_weight[np.where(y_train == i)] = num_class[i]\n",
    "    \n",
    "    clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        \n",
    "    predicted = clf.predict_proba(X_test)\n",
    "    # 取出正類（index 1）的概率\n",
    "    predicted = [predicted[i][1] for i in range(len(predicted))]\n",
    "\n",
    "        \n",
    "    num_groups = len(predicted) // group_size \n",
    "    if sum(predicted[:group_size]) / group_size > 0.5:\n",
    "        y_pred = [max(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "    else:\n",
    "        y_pred = [min(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "        \n",
    "        \n",
    "    y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "    \n",
    "    auc_score = roc_auc_score(y_test_agg, y_pred, average='micro')\n",
    "    print(f'ROC {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ffca8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義多類別分類評分函數 (例如 play years、level)\n",
    "def model_multiary(X_train, y_train, X_test, y_test, group_size=27, n_estimators=100, classifier=None):\n",
    "    # clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    # clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
    "    if classifier == 'gradient':\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)\n",
    "        num_class = []\n",
    "        for i in range(max(y_train) + 1):\n",
    "            num_class.append((y_train == i).sum())\n",
    "        num_class = {i: len(y_train) / num for i, num in enumerate(num_class)}\n",
    "        sample_weight = np.zeros(len(y_train))\n",
    "        for i in range(max(y_train) + 1):\n",
    "            sample_weight[np.where(y_train == i)] = num_class[i]\n",
    "        \n",
    "        clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "    else:\n",
    "        clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, class_weight='balanced')\n",
    "        clf.fit(X_train, y_train)\n",
    "    # clf = HistGradientBoostingClassifier(random_state=42)\n",
    "    # clf = MLPClassifier()\n",
    "    predicted = clf.predict_proba(X_test)\n",
    "    num_groups = len(predicted) // group_size\n",
    "    y_pred = []\n",
    "    for i in range(num_groups):\n",
    "        group_pred = predicted[i*group_size: (i+1)*group_size]\n",
    "        num_classes = len(np.unique(y_train))\n",
    "        # 對每個類別計算該組內的總機率\n",
    "        class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n",
    "        chosen_class = np.argmax(class_sums)\n",
    "        candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n",
    "        best_instance = np.argmax(candidate_probs)\n",
    "        y_pred.append(group_pred[best_instance])\n",
    "        \n",
    "    y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "    auc_score = roc_auc_score(y_test_agg, y_pred, average='micro', multi_class='ovr')\n",
    "    print(f'Multiary AUC: {auc_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce442c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 若尚未產生特徵，請先執行 data_generate() 生成特徵 CSV 檔案\n",
    "    # data_generate()\n",
    "    \n",
    "    # 讀取訓練資訊，根據 player_id 將資料分成 80% 訓練、20% 測試\n",
    "    info = pd.read_csv('train_info.csv')\n",
    "    unique_players = info['player_id'].unique()\n",
    "    train_players, test_players = train_test_split(unique_players, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 讀取特徵 CSV 檔（位於 \"./tabular_data_train\"）\n",
    "    datapath = './tabular_data_train'\n",
    "    datalist = list(Path(datapath).glob('**/*.csv'))\n",
    "    target_mask = ['gender', 'hold racket handed', 'play years', 'level']\n",
    "    \n",
    "    # 根據 test_players 分組資料\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.DataFrame(columns=target_mask)\n",
    "    X_test = pd.DataFrame()\n",
    "    y_test = pd.DataFrame(columns=target_mask)\n",
    "    \n",
    "    for file in datalist:\n",
    "        unique_id = int(Path(file).stem)\n",
    "        row = info[info['unique_id'] == unique_id]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        player_id = row['player_id'].iloc[0]\n",
    "        data = pd.read_csv(file)\n",
    "        mode = info.loc[info['unique_id'] == unique_id, 'mode'].values[0] # a scalar\n",
    "        mode_onehot = np.zeros((1))\n",
    "        mode_onehot[0] = 1 if mode >= 9 else 0\n",
    "        # mode_onehot[mode - 1] = 1 # mode : 1-10\n",
    "        mode_onehot = pd.DataFrame([mode_onehot] * len(data))\n",
    "        # from GPT ==================================\n",
    "        # 查看目前有幾列\n",
    "        num_rows = len(data)\n",
    "\n",
    "        # 如果不足 27 列，就補 0 到達 27 列\n",
    "        if num_rows < 27:\n",
    "            print('bug')\n",
    "            num_missing = 27 - num_rows\n",
    "            missing_rows = pd.DataFrame(0, index=range(num_missing), columns=data.columns)\n",
    "            data = pd.concat([data, missing_rows], ignore_index=True)\n",
    "        # from GPT ==================================\n",
    "\n",
    "        target = row[target_mask]\n",
    "        target_repeated = pd.concat([target] * len(data))\n",
    "        data = pd.concat([data, mode_onehot], axis=1)\n",
    "        if player_id in train_players:\n",
    "            X_train = pd.concat([X_train, data], ignore_index=True)\n",
    "            y_train = pd.concat([y_train, target_repeated], ignore_index=True)\n",
    "        elif player_id in test_players:\n",
    "            X_test = pd.concat([X_test, data], ignore_index=True)\n",
    "            y_test = pd.concat([y_test, target_repeated], ignore_index=True)\n",
    "    print('train shape', X_train.shape)\n",
    "    print('test shape', X_test.shape)\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "    \n",
    "    # 標準化特徵\n",
    "    le = LabelEncoder()\n",
    "    def normalize(name, bound=10, mode=0):\n",
    "        X_corr = X_train.apply(lambda col: col.corr(y_train[name]))\n",
    "        X_corr = X_corr.sort_values(ascending=False)\n",
    "        \n",
    "        if mode == 1: # pos\n",
    "            columns = X_corr.head(bound).index.tolist()\n",
    "        elif mode == 2: # neg\n",
    "            columns = X_corr.tail(bound).index.tolist()\n",
    "        \n",
    "        if mode == 0:\n",
    "            X_train_func = X_train\n",
    "            X_test_func = X_test\n",
    "        elif mode == 1:\n",
    "            X_train_func = X_train[columns]\n",
    "            X_test_func = X_test[columns]\n",
    "        elif mode == 2:\n",
    "            X_train_func = X_train.drop(columns=columns)\n",
    "            X_test_func = X_test.drop(columns=columns)\n",
    "            \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_func)\n",
    "        X_test_scaled = scaler.transform(X_test_func)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "    # =====================================================================================\n",
    "    # display_corr(X_train, y_train, 'level')\n",
    "\n",
    "    # 評分：針對各目標進行模型訓練與評分\n",
    "    X_train_scaled, X_test_scaled = normalize('gender', 10, 2)\n",
    "    y_train_le_gender = le.fit_transform(y_train['gender'])\n",
    "    y_test_le_gender = le.transform(y_test['gender'])\n",
    "    model_binary(X_train_scaled, y_train_le_gender, X_test_scaled, y_test_le_gender, group_size=27)\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('hold racket handed', mode=0)\n",
    "    y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n",
    "    y_test_le_hold = le.transform(y_test['hold racket handed'])\n",
    "    model_binary(X_train_scaled, y_train_le_hold, X_test_scaled, y_test_le_hold, group_size=27)\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('play years', 10, mode=0)\n",
    "    y_train_le_years = le.fit_transform(y_train['play years'])\n",
    "    y_test_le_years = le.transform(y_test['play years'])\n",
    "    model_multiary(X_train_scaled, y_train_le_years, X_test_scaled, y_test_le_years, group_size=27, classifier='')\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('level', 10, mode=0)\n",
    "    y_train_le_level = le.fit_transform(y_train['level'])\n",
    "    y_test_le_level = le.transform(y_test['level'])\n",
    "    model_multiary(X_train_scaled, y_train_le_level, X_test_scaled, y_test_le_level, group_size=27, n_estimators=1400)\n",
    "\n",
    "    #AUC SCORE: 0.792(gender) + 0.998(hold) + 0.660(years) + 0.822(levels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd01e12f",
   "metadata": {},
   "source": [
    " - Test 好像有缺失值\n",
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6642065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_generate('./train_data', 'tabular_data_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ad7706e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (42039, 35)\n",
      "test shape (10746, 35)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[134], line 107\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m y_train_le_level \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mfit_transform(y_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    106\u001b[0m y_test_le_level \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39mtransform(y_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 107\u001b[0m \u001b[43mmodel_multiary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_le_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_le_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m27\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1450\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[133], line 18\u001b[0m, in \u001b[0;36mmodel_multiary\u001b[1;34m(X_train, y_train, X_test, y_test, group_size, n_estimators, classifier)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     clf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# clf = HistGradientBoostingClassifier(random_state=42)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# clf = MLPClassifier()\u001b[39;00m\n\u001b[0;32m     21\u001b[0m predicted \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[0;32m    206\u001b[0m     )\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e99f1",
   "metadata": {},
   "source": [
    "## Predict Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "987b133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submit():   \n",
    "    train_target_mask = ['gender', 'hold racket handed', 'play years', 'level']\n",
    "    test_target_mask = ['gender', 'hold racket handed']\n",
    "    \n",
    "    # Train Data\n",
    "    train_datapath = './tabular_data_train'\n",
    "    train_datalist = list(Path(train_datapath).glob('**/*.csv'))\n",
    "    \n",
    "    train_info = pd.read_csv('train_info.csv')\n",
    "    train_players = train_info['player_id'].unique()\n",
    "    \n",
    "    # Test Data\n",
    "    test_datapath = './tabular_data_test'\n",
    "    test_datalist = list(Path(test_datapath).glob('**/*.csv'))\n",
    "    \n",
    "    test_info = pd.read_csv('test_info.csv')\n",
    "    test_players = test_info['unique_id']\n",
    "    \n",
    "    # 根據 test_players 分組資料\n",
    "    X_train = pd.DataFrame()\n",
    "    y_train = pd.DataFrame(columns=train_target_mask)\n",
    "    X_test = pd.DataFrame()\n",
    "    y_test = pd.DataFrame(columns=['unique_id'] + test_target_mask)\n",
    "    y_test['unique_id'] = test_players\n",
    "    \n",
    "    # Make Train DF\n",
    "    for file in train_datalist:\n",
    "        unique_id = int(Path(file).stem)\n",
    "        row = train_info[train_info['unique_id'] == unique_id]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        # player_id = row['player_id'].iloc[0]\n",
    "        data = pd.read_csv(file)\n",
    "        \n",
    "        mode = train_info.loc[train_info['unique_id'] == unique_id, 'mode'].values[0] # a scalar\n",
    "        mode_onehot = np.zeros((1))\n",
    "        mode_onehot[0] = 1 if mode >= 9 else 0\n",
    "        mode_onehot = pd.DataFrame([mode_onehot] * len(data))\n",
    "        \n",
    "        target = row[train_target_mask]\n",
    "        target_repeated = pd.concat([target] * len(data))\n",
    "        data = pd.concat([data, mode_onehot], axis=1)\n",
    "        X_train = pd.concat([X_train, data], ignore_index=True)\n",
    "        y_train = pd.concat([y_train, target_repeated], ignore_index=True)\n",
    "\n",
    "    # Make Test DF\n",
    "    for file in test_datalist:\n",
    "        unique_id = int(Path(file).stem)\n",
    "        row = test_info[test_info['unique_id'] == unique_id]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        # player_id = row['player_id'].iloc[0]\n",
    "        data = pd.read_csv(file)\n",
    "        if data.empty:\n",
    "            print(file)\n",
    "            \n",
    "        mode = test_info.loc[test_info['unique_id'] == unique_id, 'mode'].values[0] # a scalar\n",
    "        mode_onehot = np.zeros((1))\n",
    "        mode_onehot[0] = 1 if mode >= 9 else 0\n",
    "        mode_onehot = pd.DataFrame([mode_onehot] * len(data))\n",
    "        \n",
    "        # target = row[target_mask]\n",
    "        # target_repeated = pd.concat([target] * len(data))\n",
    "        data = pd.concat([data, mode_onehot], axis=1)\n",
    "        X_test = pd.concat([X_test, data], ignore_index=True)\n",
    "        # y_test = pd.concat([y_test, target_repeated], ignore_index=True)\n",
    "    print(X_train.shape)\n",
    "    print(X_test.shape)\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    def normalize(name, bound=10, mode=0):\n",
    "        X_corr = X_train.apply(lambda col: col.corr(y_train[name]))\n",
    "        X_corr = X_corr.sort_values(ascending=False)\n",
    "        \n",
    "        if mode == 1: # pos\n",
    "            columns = X_corr.head(bound).index.tolist()\n",
    "        elif mode == 2: # neg\n",
    "            columns = X_corr.tail(bound).index.tolist()\n",
    "        \n",
    "        if mode == 0:\n",
    "            X_train_func = X_train\n",
    "            X_test_func = X_test\n",
    "        elif mode == 1:\n",
    "            X_train_func = X_train[columns]\n",
    "            X_test_func = X_test[columns]\n",
    "        elif mode == 2:\n",
    "            X_train_func = X_train.drop(columns=columns)\n",
    "            X_test_func = X_test.drop(columns=columns)\n",
    "            \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_func)\n",
    "        X_test_scaled = scaler.transform(X_test_func)\n",
    "        return X_train_scaled, X_test_scaled\n",
    "    # =====================================================================================\n",
    "    def model_binary_pred(X_train, y_train, X_test, y_test, name, group_size=27, n_estimators=100):\n",
    "        # clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, class_weight='balanced')\n",
    "        clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)\n",
    "        num_class = [(y_train == 0).sum(), (y_train == 1).sum()]\n",
    "        num_class = {i: len(y_train) / num for i, num in enumerate(num_class)}\n",
    "        sample_weight = np.zeros(len(y_train))\n",
    "        for i in range(2):\n",
    "            sample_weight[np.where(y_train == i)] = num_class[i]\n",
    "        \n",
    "        clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        \n",
    "        predicted = clf.predict_proba(X_test)\n",
    "        # 取出正類（index 1）的概率\n",
    "        predicted = [1 - predicted[i][1] for i in range(len(predicted))]\n",
    "\n",
    "        \n",
    "        num_groups = len(predicted) // group_size \n",
    "        # print('num group', num_groups)\n",
    "        if sum(predicted[:group_size]) / group_size > 0.5:\n",
    "            y_pred = [max(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "        else:\n",
    "            y_pred = [min(predicted[i*group_size: (i+1)*group_size]) for i in range(num_groups)]\n",
    "            \n",
    "        # y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "        y_test[name] = y_pred\n",
    "        \n",
    "    # 定義多類別分類評分函數 (例如 play years、level)\n",
    "    def model_multiary_pred(X_train, y_train, X_test, y_test, name:list, group_size=27, n_estimators=100, classifier=None):\n",
    "        if classifier == 'gradient':\n",
    "            clf = GradientBoostingClassifier(n_estimators=n_estimators, random_state=42)\n",
    "            num_class = []\n",
    "            for i in range(max(y_train) + 1):\n",
    "                num_class.append((y_train == i).sum())\n",
    "            num_class = {i: len(y_train) / num for i, num in enumerate(num_class)}\n",
    "            print(num_class)\n",
    "            sample_weight = np.zeros(len(y_train))\n",
    "            for i in range(max(y_train) + 1):\n",
    "                sample_weight[np.where(y_train == i)] = num_class[i]\n",
    "            \n",
    "            clf.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        else:\n",
    "            clf = RandomForestClassifier(n_estimators=n_estimators, random_state=42, class_weight='balanced')\n",
    "            clf.fit(X_train, y_train)\n",
    "        predicted = clf.predict_proba(X_test)\n",
    "        num_groups = len(predicted) // group_size\n",
    "        y_pred = []\n",
    "        for i in range(num_groups):\n",
    "            group_pred = predicted[i*group_size: (i+1)*group_size]\n",
    "            num_classes = len(np.unique(y_train))\n",
    "            # 對每個類別計算該組內的總機率\n",
    "            class_sums = [sum([group_pred[k][j] for k in range(group_size)]) for j in range(num_classes)]\n",
    "            chosen_class = np.argmax(class_sums)\n",
    "            candidate_probs = [group_pred[k][chosen_class] for k in range(group_size)]\n",
    "            best_instance = np.argmax(candidate_probs)\n",
    "            y_pred.append(group_pred[best_instance])\n",
    "        y_pred = np.array(y_pred)\n",
    "        # print(y_pred.shape)\n",
    "        for i, n in enumerate(name):\n",
    "            y_test[n] = y_pred[:, i]\n",
    "        \n",
    "        # y_test_agg = [y_test[i*group_size] for i in range(num_groups)]\n",
    "        # auc_score = roc_auc_score(y_test_agg, y_pred, average='micro', multi_class='ovr')\n",
    "        # print('Multiary AUC:', auc_score)\n",
    "        \n",
    "    # =====================================================================================\n",
    "    # 評分：針對各目標進行模型訓練與評分\n",
    "    print('Start Prediction')\n",
    "    X_train_scaled, X_test_scaled = normalize('gender', 10, 2)\n",
    "    y_train_le_gender = le.fit_transform(y_train['gender'])\n",
    "    model_binary_pred(X_train_scaled, y_train_le_gender, X_test_scaled, y_test, 'gender', group_size=27)\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('hold racket handed', mode=0)\n",
    "    y_train_le_hold = le.fit_transform(y_train['hold racket handed'])\n",
    "    model_binary_pred(X_train_scaled, y_train_le_hold, X_test_scaled, y_test, 'hold racket handed', group_size=27)\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('play years', 10, mode=0)\n",
    "    y_train_le_years = le.fit_transform(y_train['play years'])\n",
    "    labels = ['play years_0', 'play years_1', 'play years_2']\n",
    "    model_multiary_pred(X_train_scaled, y_train_le_years, X_test_scaled, y_test, labels, group_size=27, classifier='')\n",
    "    \n",
    "    X_train_scaled, X_test_scaled = normalize('level', 10, mode=0)\n",
    "    y_train_le_level = le.fit_transform(y_train['level'])\n",
    "    labels = ['level_2', 'level_3', 'level_4', 'level_5']\n",
    "    model_multiary_pred(X_train_scaled, y_train_le_level, X_test_scaled, y_test, labels, group_size=27, n_estimators=1400)\n",
    "    \n",
    "    y_test.to_csv('submit.csv', index=False)\n",
    "    print('End')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad78a8f",
   "metadata": {},
   "source": [
    "### Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3633b7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52785, 35)\n",
      "(38610, 35)\n",
      "Start Prediction\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "make_submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
